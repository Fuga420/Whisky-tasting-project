import streamlit as st
import pandas as pd
import random 
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from janome.tokenizer import Tokenizer

# --- ãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆã“ã®é–¢æ•°ã¯å¤‰æ›´ãªã—ï¼‰ ---
@st.cache_data
def load_data_and_model():
    print("--- ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã‚’é–‹å§‹ã—ã¾ã™ ---")
    
    csv_file_path = Path('output') / 'whisky_dataset_final.csv'
    df = pd.read_csv(csv_file_path)
    
    df['å¹´æ•°'] = pd.to_numeric(df['å¹´æ•°'], errors='coerce')
    df.dropna(subset=['å•†å“å', 'ãƒ†ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒãƒ¼ãƒˆ'], inplace=True)
    df = df[df['ãƒ†ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒãƒ¼ãƒˆ'].str.strip() != '']
    df.reset_index(drop=True, inplace=True)

    t = Tokenizer()
    stop_words = ['ã™ã‚‹', 'ã„ã‚‹', 'ã‚ã‚‹', 'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã‚Œ', 'æ€ã†', 'æ„Ÿã˜', 'çš„',
                  'é¦™ã‚Š', 'å‘³ã‚ã„', 'ãƒ•ã‚£ãƒ‹ãƒƒã‚·ãƒ¥', 'ã‚¢ãƒ­ãƒ', 'ãƒ•ãƒ¬ãƒ¼ãƒãƒ¼', 'ãƒãƒ¼ãƒˆ', 'ãƒ†ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°']
    def clean_and_tokenize(text):
        words = [token.surface for token in t.tokenize(text)]
        cleaned_words = [word for word in words if word not in stop_words and len(word) > 1 and not word.isnumeric()]
        return ' '.join(cleaned_words)
    df['tokens_cleaned'] = df['ãƒ†ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒãƒ¼ãƒˆ'].apply(clean_and_tokenize)
    
    smoky_words = ['ã‚¹ãƒ¢ãƒ¼ã‚­ãƒ¼', 'ãƒ”ãƒ¼ãƒˆ', 'ãƒ”ãƒ¼ãƒ†ã‚£', 'ãƒ¨ãƒ¼ãƒ‰', 'ç…™', 'ç‡»è£½', 'æ½®', 'è–¬å“', 'æ­£éœ²ä¸¸']
    fruity_words = ['ãƒ•ãƒ«ãƒ¼ãƒ†ã‚£ãƒ¼', 'ãƒ•ãƒ«ãƒ¼ãƒ„', 'æœå®Ÿ', 'ãƒªãƒ³ã‚´', 'æŸ‘æ©˜', 'ãƒ¬ãƒ¢ãƒ³', 'ã‚ªãƒ¬ãƒ³ã‚¸', 'ãƒ™ãƒªãƒ¼', 'ãƒ¬ãƒ¼ã‚ºãƒ³', 'ãƒ”ãƒ¼ãƒ', 'ã‚¢ãƒ—ãƒªã‚³ãƒƒãƒˆ', 'ãƒ‘ã‚¤ãƒŠãƒƒãƒ—ãƒ«']
    sherry_words = ['ã‚·ã‚§ãƒªãƒ¼', 'ãƒ‰ãƒ©ã‚¤ãƒ•ãƒ«ãƒ¼ãƒ„', 'ãƒ¬ãƒ¼ã‚ºãƒ³', 'ã‚«ã‚«ã‚ª', 'ãƒãƒ§ã‚³ãƒ¬ãƒ¼ãƒˆ']
    def add_weights(text):
        if not isinstance(text, str): return ""
        boosted_text = text
        if any(word in text for word in smoky_words): boosted_text += ' ã‚¹ãƒ¢ãƒ¼ã‚­ãƒ¼' * 10
        if any(word in text for word in fruity_words): boosted_text += ' ãƒ•ãƒ«ãƒ¼ãƒ†ã‚£ãƒ¼' * 5
        if any(word in text for word in sherry_words): boosted_text += ' ã‚·ã‚§ãƒªãƒ¼' * 8
        return boosted_text
    df['tokens_boosted'] = df['tokens_cleaned'].apply(add_weights)
    
    vectorizer = TfidfVectorizer(max_features=2000)
    tfidf_matrix = vectorizer.fit_transform(df['tokens_boosted'])
    cosine_sim_matrix = cosine_similarity(tfidf_matrix)
    indices = pd.Series(df.index, index=df['å•†å“å']).drop_duplicates()
    
    print("--- ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸ ---")
    return df, cosine_sim_matrix, indices, vectorizer, tfidf_matrix

# --- ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ‰é–¢æ•°ï¼ˆå¤‰æ›´ãªã—ï¼‰ ---
def get_recommendations_with_serendipity(title, df, cosine_sim, indices):
    try:
        idx = indices[title]
    except KeyError: return pd.DataFrame()
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    top_candidates_indices = [i[0] for i in sim_scores[1:31]]
    if len(top_candidates_indices) < 5: return df.iloc[top_candidates_indices]
    final_recommendations_indices = top_candidates_indices[:2]
    remaining_candidates = top_candidates_indices[2:]
    random_sample_indices = random.sample(remaining_candidates, 3)
    final_recommendations_indices.extend(random_sample_indices)
    return df.iloc[final_recommendations_indices]

def recommend_by_flavor(keywords, vectorizer, tfidf_matrix, df, top_n=5):
    keyword_vec = vectorizer.transform(keywords)
    cosine_sim = cosine_similarity(keyword_vec, tfidf_matrix)
    top_indices = cosine_sim.flatten().argsort()[::-1][:top_n]
    return df.iloc[top_indices]

# --- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®UIï¼ˆè¦‹ãŸç›®ï¼‰éƒ¨åˆ† ---
st.set_page_config(layout="wide")
st.title('AIã‚¦ã‚¤ã‚¹ã‚­ãƒ¼ã‚½ãƒ ãƒªã‚¨ ğŸ¥ƒ')
st.write('ã‚ãªãŸã®å¥½ã¿ã‹ã‚‰ã€æ¬¡ã®ä¸€æœ¬ã‚’ãŠæ¢ã—ã—ã¾ã™ã€‚')

with st.spinner('AIã‚½ãƒ ãƒªã‚¨ãŒå¨æˆ¿ã§æº–å‚™ã‚’ã—ã¦ã„ã¾ã™... åˆå›èµ·å‹•ã«ã¯æ•°åˆ†ã‹ã‹ã‚Šã¾ã™ã€‚'):
    df, cosine_sim, indices, vectorizer, tfidf_matrix = load_data_and_model()

st.success('æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼ã„ã¤ã§ã‚‚ã”æ³¨æ–‡ã‚’ã©ã†ãã€‚')

# --- â–¼â–¼â–¼ ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆçµã‚Šè¾¼ã¿ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã®ä¿®æ­£ â–¼â–¼â–¼ ---
st.sidebar.header('è©³ç´°æ¤œç´¢ã‚ªãƒ—ã‚·ãƒ§ãƒ³')

# ç†Ÿæˆå¹´æ•°ã®ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼
min_age_in_data = int(df['å¹´æ•°'].dropna().min())
max_age_in_data = int(df['å¹´æ•°'].dropna().max())
selected_age = st.sidebar.slider(
    'ç†Ÿæˆå¹´æ•°ã®ç¯„å›²',
    min_value=min_age_in_data,
    max_value=max_age_in_data,
    value=(min_age_in_data, max_age_in_data)
)

# åœ°åŸŸã®ãƒãƒ«ãƒã‚»ãƒ¬ã‚¯ãƒˆï¼ˆæŒ‡å®šã•ã‚ŒãŸãƒªã‚¹ãƒˆã«å¤‰æ›´ï¼‰
region_list = ['å…¨åœ°åŸŸ', 'ã‚¢ã‚¤ãƒ©', 'ã‚­ãƒ£ãƒ³ãƒ™ãƒ«ã‚¿ã‚¦ãƒ³', 'ã‚¹ãƒšã‚¤ã‚µã‚¤ãƒ‰', 'ã‚¢ã‚¤ãƒ©ãƒ³ã‚º', 'ãƒã‚¤ãƒ©ãƒ³ãƒ‰', 'ãƒ­ãƒ¼ãƒ©ãƒ³ãƒ‰']
selected_regions = st.sidebar.multiselect(
    'ç”Ÿç”£åœ°åŸŸï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰',
    region_list
)

# ãƒ–ãƒ©ãƒ³ãƒ‰ã®ãƒãƒ«ãƒã‚»ãƒ¬ã‚¯ãƒˆã¯ã“ã“ã‹ã‚‰å‰Šé™¤ã—ã¾ã—ãŸ

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ï¼ˆæ¤œç´¢æ©Ÿèƒ½ï¼‰ ---
st.markdown("---")
left_column, right_column = st.columns(2)

with left_column:
    st.header('å¥½ããªéŠ˜æŸ„ã‹ã‚‰æ¢ã™')
    whisky_list = df['å•†å“å'].tolist()[::-1]
    selected_whisky = st.selectbox('å¥½ããªã‚¦ã‚¤ã‚¹ã‚­ãƒ¼ã‚’1æœ¬é¸ã‚“ã§ãã ã•ã„', whisky_list, key='item_select')
    if st.button('ã“ã®ã‚¦ã‚¤ã‚¹ã‚­ãƒ¼ã«ä¼¼ãŸãŠé…’ã‚’æ¢ã™', key='item_button'):
        recommendations = get_recommendations_with_serendipity(selected_whisky, df, cosine_sim, indices)
        st.session_state['recommendations'] = recommendations
        st.session_state['search_title'] = f'ã€Œ{selected_whisky}ã€ãŒå¥½ããªã‚ãªãŸã¸ã®ãŠã™ã™ã‚'

with right_column:
    st.header('å¥½ããªé¦™å‘³ã‹ã‚‰æ¢ã™')
    flavor_input = st.text_input('å¥½ããªé¦™å‘³ã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„', placeholder='ä¾‹ï¼šãƒ•ãƒ«ãƒ¼ãƒ†ã‚£ãƒ¼, ç”˜ã„, å°‘ã—ã‚¹ãƒ¢ãƒ¼ã‚­ãƒ¼', key='flavor_input')
    if st.button('ã“ã®é¦™å‘³ã«è¿‘ã„ãŠé…’ã‚’æ¢ã™', key='flavor_button'):
        if flavor_input:
            recommendations = recommend_by_flavor([flavor_input], vectorizer, tfidf_matrix, df)
            st.session_state['recommendations'] = recommendations
            st.session_state['search_title'] = f'ã€Œ{flavor_input}ã€ã®ã‚¤ãƒ¡ãƒ¼ã‚¸ã«è¿‘ã„ãŠã™ã™ã‚'

# --- çµæœè¡¨ç¤ºã‚¨ãƒªã‚¢ ---
if 'recommendations' in st.session_state:
    st.markdown("---")
    st.header('AIã‹ã‚‰ã®ã”ææ¡ˆ')
    
    recommendations_df = st.session_state['recommendations']
    filtered_results = recommendations_df.copy()

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†
    filtered_results.dropna(subset=['å¹´æ•°'], inplace=True)
    filtered_results = filtered_results[(filtered_results['å¹´æ•°'] >= selected_age[0]) & (filtered_results['å¹´æ•°'] <= selected_age[1])]
    if selected_regions and 'å…¨åœ°åŸŸ' not in selected_regions:
        filtered_results = filtered_results[filtered_results['åœ°åŸŸ'].isin(selected_regions)]
    
    # ãƒ–ãƒ©ãƒ³ãƒ‰ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã‚’ã“ã“ã‹ã‚‰å‰Šé™¤ã—ã¾ã—ãŸ

    st.subheader(st.session_state['search_title'])
    
    if filtered_results.empty:
        st.warning('æ¡ä»¶ã«åˆã†ã‚¦ã‚¤ã‚¹ã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚çµã‚Šè¾¼ã¿æ¡ä»¶ã‚’ç·©ã‚ã¦ã¿ã¦ãã ã•ã„ã€‚')
    else:
        for index, row in filtered_results.iterrows():
            with st.container(border=True):
                col1, col2 = st.columns([1, 2])
                with col1:
                    if 'ç”»åƒURL' in row and pd.notna(row['ç”»åƒURL']) and row['ç”»åƒURL'] != 'URLãªã—':
                        st.image(row['ç”»åƒURL'], width=150)
                with col2:
                    st.markdown(f"#### {row['å•†å“å']}")
                    if 'ãƒ–ãƒ©ãƒ³ãƒ‰' in row and pd.notna(row['ãƒ–ãƒ©ãƒ³ãƒ‰']):
                        st.markdown(f"**ãƒ–ãƒ©ãƒ³ãƒ‰:** {row.get('ãƒ–ãƒ©ãƒ³ãƒ‰', 'æƒ…å ±ãªã—')} | **åœ°åŸŸ:** {row.get('åœ°åŸŸ', 'æƒ…å ±ãªã—')}")
                    with st.expander("ãƒ†ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒãƒ¼ãƒˆã‚’è¦‹ã‚‹"):
                        st.write(row.get('ãƒ†ã‚¤ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒãƒ¼ãƒˆ', 'æƒ…å ±ãªã—'))
                    st.markdown(f"[å•†å“ãƒšãƒ¼ã‚¸ã¸]({row.get('URL', '#')})")